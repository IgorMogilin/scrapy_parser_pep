# scrapy_parser_pep

## Оглавление

- [Автор](#автор)  
- [Используемые технологии](#используемые-технологии)  
- [Описание проекта](#описание-проекта)  
- [Установка и запуск](#установка-и-запуск)  
- [Примеры использования](#примеры-использования)  

---

## Используемые технологии

- Python 3.9.10  
- Scrapy (фреймворк для асинхронного веб-скрейпинга)
- CSV (сохранение результатов в формате CSV)
- pathlib (работа с путями и директориями)

---

## Описание проекта

Финальный проект спринта — асинхронный парсер документов PEP с использованием Scrapy.
Задача проекта — научиться создавать и настраивать Scrapy-пауков, которые:

- парсят список PEP  
- собирают данные по каждому документу (номер, название, статус)
- сохраняют результаты в CSV файлы в директорию results (путь берётся из настройки FEEDS)
- генерируют статистику по статусам PEP и сохраняют её отдельным CSV файлом

Проект демонстрирует основы работы с Scrapy: пауки, пайплайны, настройки, экспорт данных и обработку результатов.

---

## Установка и запуск

1. Клонируйте репозиторий и перейдите в папку проекта:  
   ```bash
   git clone https://github.com/IgorMogilin/scrapy_parser_pep.git
   cd scrapy_parser_pep
   ```


Создайте и активируйте виртуальное окружение:

2. Для Linux/macOS:
   ```bash
    python -m venv venv
    source venv/bin/activate
   ```


   Для Windows:
   ```bash
    python -m venv venv
    venv\Scripts\activate
   ```
3. Обновите pip:
   ```bash
    python -m pip install --upgrade pip
   ```
4. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

# Примеры использования

### Запуск паука и сбор данных::

   ```bash
   scrapy crawl pep
   ```

## Автор

- [Могилин Игорь](https://github.com/IgorMogilin)
